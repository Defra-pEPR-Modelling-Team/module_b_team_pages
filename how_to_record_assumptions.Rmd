---
title: "How to Record Assumptions" 
---

Follow these instructions to identify and record assumptions, for more guidance on what exactly is an assumption, scroll to the background section below.

# Instructions 

1.    **Identify an assumption: ** 

        Go through your code and identify modelling choices you have made- ask yourself the following questions: 

        -   Are there other choices you could have made? 

        -   Would changing your choice affect the final number? 

        -   Can you imagine your choice leading to the wrong number? 

        -   Can you justify your choice? 

        If you answered ‘Yes’ to any two of the above questions, your modelling decision is an assumption. 

2.    **Record your assumption in plain English, together with its location and approximate area in the code, in the “Assumptions Log” tab of the relevant QA Log:**

        ![](~/module_b_team_pages/assets/qa_log_columns.png)
    
3.    **Rate your assumption for Impact: **

        These are rated 
        -   RED = Critical Impact, 
        -   AMBER = Medium Impact and 
        -   GREEN = Limited Impact. 
    
        You can assess this directly quantitatively (see step 5), but the following rules of thumb may be useful: 

        -   Choices of datasets, variables and key formulas have a Critical Impact, because they directly determine the number for all outputs. 
        -   Treatment of individual cases which appear relatively often have a Medium Impact because they determine some of the outputs or a decent portion of the number. 
        -   Treatment of edge cases, rare NAs etc have a Limited Impact because they only affect the outputs to a very limited degree. 

4.    **Rate your assumption for Quality: **

        These are rated 
        -   RED = Low Quality, 
        -   AMBER = Mid Quality and 
        -   GREEN = High Quality.
    
        Again see step 5 for guidance.

5.    **Explain your choices of Impact and Quality RAG in the “Comments” box: **

        Impact: 
        -   Critical impact does not need to be justified for choice of datasets, variables, and key formulas. 
        -   Where the assumption applies to a subset of cases, the percentage of such cases among the total should be calculated and recorded here. 
        -   Fixes for missing or zero numbers should be quantified by a percentage increase, also recorded here. 
        -   As a rule of thumb Critical Impact is an effect of > 5%, Medium Impact is an effect of 1-5% and Limited Impact is an effect of <1%. 

        Quality: 

        What constitutes assumption quality is a hard question, depending on the kind of assumption, and the level of impact. We should record: 
        -   Is the effect of the assumption well understood? (are there graphs to help validate it?) 
        -   Have there been any hypothesis tests to justify it? 
        -   Has the assumption been scrutinised by stakeholders Eg. Technical Working Group? 
        -   (if a dataset) How old are the data? 
        -   (if a dataset) Were the data collected using a replicable, unbiased methodology? 
        -   (if a dataset) Do the data cover all the population? 
        -   Is there existing quality assurance? If so, how old is it and how good is it? 

        A high impact assumption should be assessed thoroughly on all the above that are relevant. A lower impact assumption may require a less thorough assessment. The level of the assessment, as well as the outcome, remains a matter of judgement. The following rules of thumb may be useful however: 

        -   High Quality denotes a reliable assumption, well understood and/or documented; anything up to a validated & recent set of actual data. 
        -   Mid Quality means there is some evidence to support the assumption, which may vary from a source with poor methodology to a good source that is a few years old. 
        -   Low Quality means there is little evidence to support the assumption, which may vary from an opinion to a limited data source with poor methodology. 

6.    **Save any code used to validate and or quantify the impact of the assumption, as well as any outputs: **

        Code should go in the documentation folder on github.

        Outputs of this validation code can be pasted into the outputs tab of the spreadsheet 

        Include any links to other documentation e.g. 

        -   Existing QA reports for datasets 
        -   Technical working group minutes 
        -   Emails verifying with stakeholders 
        -   For high impact assumptions- separate QA logs for supporting analysis 

7.    **Transfer the records from steps 2-7 into your code:**

        Paste the following code snippet next to the relevant portion of code and edit to reflect the above. The location or area need not be included as a separate detail, as these are implied by location in the code. The description will need to be extended over multiple lines to contain all comments. 
```{r, eval = FALSE}
    # Assumption: Title of assumption
    # Quality: RED 
    # Impact: AMBER 
    # Detailed description 
    # on next line or many. 
```

# Background

If you can imagine testifying to a judicial review about why you made a modelling choice, then it’s an assumption.

As analysts, we make choices with every line of code. In some cases, these do not materially affect the output of the model: reformatting columns, for example, might make the code more user friendly, but shouldn’t change the output. But wherever a **choice materially changes the output**, whether that is a choice of column, a fix for missing values, or the use of a formula, that is an assumption, and we (that is Defra potentially long after you have been headhunted away by Google) **need to be able to say why you made it.**

Documenting assumptions completely can be quite an arduous task, and theoretically they represent half of how your code interfaces with the real world.

But assumptions come in a spectrum of importance:

![](~/module_b_team_pages/assets/assumption_hierarchy.png)

…Of course, sometimes highly obvious assumptions can turn out to be wrong. I once heard of a pension fund who supplied all their eligible years data as “3.152” meaning “3 years, 152 days”. Understandably nobody using the data recorded the assumption that it was a decimal…
