---
title: "Learning"
---

Here are a number of resources you can use to help improve your coding:

  - [Defra's monthly data science sessions](https://defra.sharepoint.com/sites/Community448/SitePages/Events.aspx)
  - [BEIS's weekly Coffee and Coding sessions](https://forms.office.com/pages/responsepage.aspx?id=BXCsy8EC60O0l-ZJLRst2KUXyeG5QzlKhk--pMPwnKxURTJXTTFZVjVQSFA2SE1ZSVNKRDE3NzBYTy4u)
  - Defra has unlimited [DataCamp](https://www.datacamp.com/) licenses, so anyone in Defra can sign up for a pro account
  
# ONS Guidance

Generally, the [Duck Book](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html) should inform how we work.

The book's guidance is embedded in these pages but here are some important points to peruse:

### Theory

- The three pillars of rigorous analysis...
    - Reproducible analysis: "the same analysis steps performed on the same dataset consistently produces the same result"
    - Auditable analysis: "analysis and supporting evidence are available for scrutiny"
    - Assured analysis: "analysis has passed through a systematic process that established it as fit for purpose"
- On good coding practice...
    - Despite the initial cost of developing technical skills, **evidence shows that applying good practices increases the efficiency of code development and maintainability of the code**. There are number of case studies that describe how good quality assurance practices have improved government analysis.
    - Not following good practices creates technical debt, which slows down further development of the analysis. This can be necessary for delivering to short deadlines, but time should be set aside to address this for continued development of the analysis.

### Practice

- On documentation...
    - Pipelines should be well documented, and that **documentation should be embedded in our code**. This makes collaboration easier, as well as allowing for easier auditing and assurance. When documentation exists alongside the code, it is more likely to be kept up to date than if it is spread out across different systems. Having the documentation as part of the code means we have a single source of the truth.
    - Every function should be documented in the code, so that it is clear what the function is supposed to do. Function documentation should include what goes in and what comes out of each function.
    - Assumptions and caveats of the analysis should be recorded close to the code.
    - When working on a collaborative or open coding project, it’s good practice to describe an overview of your project in a README file. This allows users or developers to grasp the overall goal of your project. 
    - Note: could include specification diagram like in QA log "map/pseudocode"
- On modularity...
    - This means breaking your code down into self-contained elements, for example [subscripts], classes, and functions. **Functions are the lowest level of our code. They should do one thing and do it well. [Subscripts] are collections of related functions. We can run our pipeline from a high level script that calls in functions from our [subscripts].**
    - Note: have substituted 'subscripts' for modules to avoid confusion with 'module B'.
    - Logic should be written as functions, so that it can be reused consistently and tested. Related functions should be grouped together in the same file, so that it is easy to find them. Logic with different responsibilities (e.g. reading data versus transforming data) should be clearly separated.
    - Note: running scripts in this way ensures your code is executed reproducibly
- On QA...
    - We should automate the aspects of quality assurance that are challenging for humans. This frees up time and makes it easier to keep a record of what QA has been performed. The purpose of this automation is not to get rid of analysts, but to allow analysts to focus on the QA that computers cannot easily do. For example, analysts are much better at ensuring that the analysis is fit for purpose.      
    - As described by the Aqua book, the **quality assurance of our analysis should be proportional to the complexity and risk of the analysis**.
    - Where quality assurance of the code doesn’t meet your target level of assurance, for example where there is limited time or skill, then it is necessary to supplement this with more in-depth assurance of outputs. This might include dual running the analysis with an independent system and consistency checks across the output data.
- On Agile working...    
    - **In Agile, working software is considered the primary measure of success.** This means that we don’t spend months creating detailed plans and documenting specifications. Instead, we prioritise early and continuous delivery of working software. 
- On data...
    - **You should not alter raw data** - treat it as read-only. Even data cleaning should take place on a copy of the raw data, so that you can document which cleaning decisions have been made. There must be an immutable store for raw data in your project structure.
- On version control...
    - **Commit small and often**
    - We should 'include code **and documentation** in version control.

### For the future

- On unit testing...
    - In their simplest form, software tests run a part of the pipeline on example data. They assert that the output data meets certain expectations. If these expectations are not met, an error will be raised. The analyst can then investigate what caused this error. Modularising our code makes it much easier to check that it is working as expected. For each important or complex function, we should write at least one test. 
    - Automated ‘unit’ testing should be applied to each function, to ensure that code continues to work after future changes to the code.
    - Each function and the end-to-end analysis should be tested using minimal, realistic data.
    - Testing should be more extensive on the most important or complex parts of the code. However, ideally every single function should be tested.
    - Tests should account for realistic cases, which might include missing data, zeroes, infinities, negative numbers, wrong data types, and invalid inputs.
    - Each time an error is found in the code, a test should be added to assure that the error does not reoccur.
- On developing models over time...
    - When repeating [or developing] analysis over time, you should compare results between analyses. Large differences in the outcome of the results may indicate an issue with the analysis process [and allow you to identify differences and improvements].
- On error catching...
    - When code fails to run, it should provide informative error messages to describe the problem.
    - Note: we haven't thought about error catching but is linked to unit tests.
- On reproducibility...
    - Code should not be dependent on a specific computer to run. Running it on a colleague’s system can help to check this.
    - When the same analysis is run multiple times or on different systems it should give reproducible outcomes.
    - Note: Munzer suggests `renv` to create a reproducible environment. We may also need a configuration (config) file.
- On version control...
    - To reference specific points in project’s history, Git allows us to create “tags”. These tags essentially act as an alias for a particular commit hash, allowing us to refer to it by an informative label. In analytical projects, we might use tags to mark a particular model version or an important stage of our analysis. 
    - Use Git hooks to encourage good practice 
    - Note: Git hooks can be used to automatically run lintr, stylr and unit tests
- On data...
    - Large or complex datasets should be stored in a database. Databases are collections of related data, which can be easily accessed and managed.

    
    